{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank Analysis\n",
    "\n",
    "#### Library\n",
    "- Pyspark\n",
    "- Amazon Sagemaker\n",
    "- dataclasses\n",
    "\n",
    "#### Author: Yiran Jing\n",
    "#### Date: Jan 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List\n",
    "from pyspark.sql.functions import lit\n",
    "from Rank_analysis_helperfunction import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Row\n",
    "import pyspark\n",
    "\n",
    "# only needed in sagemaker\n",
    "import sagemaker_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=Rank Model>\n",
      "CPU times: user 189 ms, sys: 9.08 ms, total: 198 ms\n",
      "Wall time: 3.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Build the SparkSession\n",
    "\"\"\"\n",
    "# getOrCreate(): get the current Spark session or to create one if there is none running\n",
    "# The cores property controls the number of concurrent tasks an executor can run. \n",
    "# Note that too high cores per executor can lead to bad I/O throughput.\n",
    "# manage Spark memory limits programmatically \n",
    "# To avoid out of memory error\n",
    "# quite broadcast join.\n",
    "# spark.executor.cores: The number of cores to use on each executor.\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "spark = SparkSession.builder \\\n",
    "   .appName(\"Rank Model\") \\\n",
    "   .config(\"spark.driver.extraClassPath\", classpath) \\\n",
    "   .getOrCreate()\n",
    " \n",
    "sc = spark.sparkContext\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configuration for Sagemaker\n",
    "\"\"\"\n",
    "# Seurity configuration \n",
    "# find them in 'Security Credentials'\n",
    "aws_access_key_id = ''\n",
    "aws_secret_access_key = ''\n",
    "\"\"\"\n",
    "Define Data Path\n",
    "\"\"\"\n",
    "region=\"ap-southeast-2\" # Sydney Specific\n",
    "bucket = 'lagarderecommercial'\n",
    "prefix = 'RankAnalysis'\n",
    "path = 's3a://{}/{}/rawData/data Ranking Report.csv'.format(bucket, prefix) # s3a means connect s3 to hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SKU: string (nullable = true)\n",
      " |-- Store: string (nullable = true)\n",
      " |-- Concept_NEW: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Index: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- BusinessUnit: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- NetSales: float (nullable = false)\n",
      " |-- rank: float (nullable = false)\n",
      " |-- range_expansion: string (nullable = true)\n",
      " |-- material_change: string (nullable = false)\n",
      " |-- unadressed_gap: string (nullable = false)\n",
      " |-- newcomer: string (nullable = false)\n",
      "\n",
      "Begin write data to CSV\n",
      "\n",
      "CPU times: user 145 ms, sys: 55.5 ms, total: 200 ms\n",
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## load and clean data\n",
    "#df = spark.read.format(\"csv\").load(path)\n",
    "df = spark.read.csv(path, header=True) # raw data \n",
    "\n",
    "# create dataclass onject\n",
    "df = clean_dataset(df)\n",
    "dataset = Dataset(df = df, store_item_concept = get_store_item_concept_list(df, spark),\n",
    "                  week = get_week_list(df), concept = get_concept_list(df))\n",
    "\n",
    "# new column calculation\n",
    "\"\"\"\n",
    "Note: \n",
    "    some functions need to run in order\n",
    "    \n",
    "    `calculate_in_W1_not_W2` must be run before `calculate_unadressed_gap` and `calculate_material_change`\n",
    "    `calculate_range_expansion` must be run before `calculate_unadressed_gap`\n",
    "    \n",
    "    Since some functions are calculated based on column `in_W1_not_W2` or `range_expansion`\n",
    "\"\"\"\n",
    "dataset.df = calculate_range_expansion(dataset)\n",
    "dataset.df = calculate_in_W1_not_W2(dataset)\n",
    "dataset.df = calculate_material_change(dataset)\n",
    "dataset.df = calculate_unadressed_gap(dataset)\n",
    "dataset.df = calculate_newcomer(dataset)\n",
    "\n",
    "# drop duplicate row and unnecessary column\n",
    "dataset.df = dataset.df.drop('avgSales_lastWeek','sumSales_oldWeek','in_W1_not_W2').dropDuplicates()\n",
    "\n",
    "dataset.df.printSchema()\n",
    "\n",
    "print(\"Begin write data to CSV\\n\")\n",
    "Outputpath = 's3a://{}/{}/output'.format(bucket, prefix) # Specific folder name only\n",
    "dataset.df.coalesce(1).write.format('com.databricks.spark.csv').option(\"header\", \"true\")\\\n",
    ".mode('overwrite').save(Outputpath)\n",
    "# dataset.df.coalesce(1).write.option(\"header\", \"true\").mode('overwrite').csv(Outputpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
