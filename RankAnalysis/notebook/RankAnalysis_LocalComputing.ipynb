{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank Analysis\n",
    "\n",
    "Local Computing\n",
    "\n",
    "#### Author: Yiran Jing\n",
    "#### Date: Jan 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List\n",
    "from pyspark.sql.functions import lit\n",
    "from Rank_analysis_helperfunction import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import findspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Row\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.9 ms, sys: 18.1 ms, total: 41 ms\n",
      "Wall time: 4.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Build the SparkSession\n",
    "\"\"\"\n",
    "findspark.init()\n",
    "# getOrCreate(): get the current Spark session or to create one if there is none running\n",
    "# The cores property controls the number of concurrent tasks an executor can run. \n",
    "# Note that too high cores per executor can lead to bad I/O throughput.\n",
    "# manage Spark memory limits programmatically \n",
    "# To avoid out of memory error\n",
    "# quite broadcast join.\n",
    "# spark.executor.cores: The number of cores to use on each executor.\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Rank Model\") \\\n",
    "   .config(\"spark.executor.cores\",1) \\\n",
    "   .getOrCreate()\n",
    " \n",
    "    \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".config(\"spark.sql.broadcastTimeout\", 600) \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SKU: string (nullable = true)\n",
      " |-- Store: string (nullable = true)\n",
      " |-- Concept_NEW: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Index: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- BusinessUnit: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- NetSales: float (nullable = false)\n",
      " |-- rank: float (nullable = false)\n",
      " |-- range_expansion: string (nullable = true)\n",
      " |-- material_change: string (nullable = false)\n",
      " |-- unadressed_gap: string (nullable = false)\n",
      " |-- newcomer: string (nullable = false)\n",
      "\n",
      "begin write data out\n",
      "CPU times: user 166 ms, sys: 47.3 ms, total: 214 ms\n",
      "Wall time: 5min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## load and clean data\n",
    "df = spark.read.csv(\"../data/rawData/data Ranking Report.csv\", header=True) # raw data \n",
    "\n",
    "# create dataclass onject\n",
    "df = clean_dataset(df)\n",
    "dataset = Dataset(df = df, store_item_concept = get_store_item_concept_list(df, spark),\n",
    "                  week = get_week_list(df), concept = get_concept_list(df))\n",
    "\n",
    "# new column calculation\n",
    "\"\"\"\n",
    "Note: \n",
    "    some functions need to run in order\n",
    "    \n",
    "    `calculate_in_W1_not_W2` must be run before `calculate_unadressed_gap` and `calculate_material_change`\n",
    "    `calculate_range_expansion` must be run before `calculate_unadressed_gap`\n",
    "    \n",
    "    Since some functions are calculated based on column `in_W1_not_W2` or `range_expansion`\n",
    "\"\"\"\n",
    "dataset.df = calculate_range_expansion(dataset)\n",
    "dataset.df = calculate_in_W1_not_W2(dataset)\n",
    "dataset.df = calculate_material_change(dataset)\n",
    "dataset.df = calculate_unadressed_gap(dataset)\n",
    "dataset.df = calculate_newcomer(dataset)\n",
    "\n",
    "# drop duplicate row and unnecessary column\n",
    "dataset.df = dataset.df.drop('avgSales_lastWeek','sumSales_oldWeek','in_W1_not_W2').dropDuplicates()\n",
    "\n",
    "dataset.df.printSchema()\n",
    "\n",
    "print(\"begin write data out\\n\")\n",
    "dataset.df.coalesce(1).write.option(\"header\", \"true\").mode('overwrite').csv(\"../data/output/result\")\n",
    "print(\"Finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test function for final output \n",
    "\n",
    "def test_no_duplicates(df: pyspark.sql.dataframe.DataFrame):\n",
    "    \"\"\"\n",
    "    If we have duplicate rows in output, it means our algorithm is incorrect or inefficient\n",
    "    \"\"\"\n",
    "    #df.createOrReplaceTempView(\"dftestView\")\n",
    "    duplicates = df.groupBy(df.columns)\\\n",
    "    .count()\\\n",
    "    .where(col('count') > 1)\\\n",
    "    .select(sum('count'))\n",
    "\n",
    "    assert duplicates.select(col('sum(count)').isNotNull()).count() ==0, \"Find some duplicates in result, might some error in the join.\"\n",
    "    \n",
    "   \n",
    "    \n",
    "def test_material_change():\n",
    "    \"\"\"\n",
    "    If material_change = True, for that (SKU, Store), we shouldnot have records in W2\n",
    "    \n",
    "    test case:\n",
    "    1.\n",
    "    AULS488089\n",
    "    AULS.AVV102\n",
    "    should be true\n",
    "    \n",
    "    2. \n",
    "    Concept_NEW  = Air N&R \n",
    "    Rank_Total = top 50\n",
    "    category = 05_\n",
    "    should have 2 true result\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    true_material_change = dataset.df.filter(col('material_change') == \"True\")\n",
    "    filter_last_data = true_material_change.filter(col('Date') == dataset.week[-1]) # records in W2\n",
    "    count = filter_last_data.count()\n",
    "    assert count ==0, 'We should have 0 records in W2, but find '+str(fcount) \n",
    "    pass\n",
    "    \n",
    "def test_outputLength():\n",
    "    pass\n",
    "\n",
    "def test_range_expansion():\n",
    "    \"\"\"\n",
    "    Should not have blank for all items sold in W2,\n",
    "    \n",
    "    test case:\n",
    "    1.\n",
    "    AUDF.CNS113\n",
    "    AUDF100430711\n",
    "    should be false \n",
    "    \n",
    "    2.\n",
    "    AULS.AVV102 \n",
    "    AULS126353\n",
    "    should be true\n",
    "    \n",
    "    3. \n",
    "    Concept_NEW  = Air N&R \n",
    "    Rank_Total = top 50\n",
    "    category = 05_\n",
    "    Company =  AULS  or  AUDF \n",
    "    should have at least 12 true result\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def test_in_W1_not_W2():\n",
    "    \"\"\"\n",
    "    AULS488089\n",
    "    AULS.AVV102\n",
    "    should be true\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
