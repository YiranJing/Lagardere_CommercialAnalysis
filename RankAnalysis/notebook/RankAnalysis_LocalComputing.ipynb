{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank Analysis\n",
    "\n",
    "Local Computing\n",
    "\n",
    "#### Author: Yiran Jing\n",
    "#### Date: Jan 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List\n",
    "from pyspark.sql.functions import lit\n",
    "from Rank_analysis_helperfunction import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import findspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from test_Rank_functions import *\n",
    "from pyspark.sql import Row\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.28 ms, sys: 1.73 ms, total: 4.01 ms\n",
      "Wall time: 12.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Build the SparkSession\n",
    "\"\"\"\n",
    "findspark.init()\n",
    "# getOrCreate(): get the current Spark session or to create one if there is none running\n",
    "# The cores property controls the number of concurrent tasks an executor can run. \n",
    "# Note that too high cores per executor can lead to bad I/O throughput.\n",
    "# manage Spark memory limits programmatically \n",
    "# To avoid out of memory error\n",
    "# quite broadcast join.\n",
    "# spark.executor.cores: The number of cores to use on each executor.\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Rank Model\") \\\n",
    "   .config(\"spark.executor.cores\",1) \\\n",
    "   .getOrCreate()\n",
    " \n",
    "    \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests\n",
      "ðŸŽ‰\ttest_in_W1_not_W2\n",
      "ðŸŽ‰\ttest_range_expansion\n",
      "ðŸŽ‰\ttest_material_change\n",
      "CPU times: user 740 ms, sys: 307 ms, total: 1.05 s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_all()  # in test_Rank_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## load and clean data\n",
    "df = spark.read.csv(\"../data/rawData/data Ranking Report.csv\", header=True) # raw data \n",
    "\n",
    "# create dataclass onject\n",
    "df = clean_dataset(df)\n",
    "dataset = Dataset(df = df, store_item_concept = get_store_item_concept_list(df, spark),\n",
    "                  week = get_week_list(df), concept = get_concept_list(df))\n",
    "\n",
    "# new column calculation\n",
    "\"\"\"\n",
    "Note: \n",
    "    some functions need to run in order\n",
    "    \n",
    "    `calculate_in_W1_not_W2` must be run before `calculate_unadressed_gap` and `calculate_material_change`\n",
    "    `calculate_range_expansion` must be run before `calculate_unadressed_gap`\n",
    "    \n",
    "    Since some functions are calculated based on column `in_W1_not_W2` or `range_expansion`\n",
    "\"\"\"\n",
    "dataset.df = calculate_range_expansion(dataset)\n",
    "dataset.df = calculate_in_W1_not_W2(dataset)\n",
    "dataset.df = calculate_material_change(dataset)\n",
    "dataset.df = calculate_unadressed_gap(dataset)\n",
    "dataset.df = calculate_newcomer(dataset)\n",
    "\n",
    "# drop duplicate row and unnecessary column\n",
    "dataset.df = dataset.df.drop('avgSales_lastWeek','sumSales_oldWeek','in_W1_not_W2').dropDuplicates()\n",
    "dataset.df.printSchema()\n",
    "\n",
    "print(\"begin write data out\\n\")\n",
    "dataset.df.coalesce(1).write.option(\"header\", \"true\").mode('overwrite').csv(\"../data/output/result\")\n",
    "print(\"Finish\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
