{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank Analysis\n",
    "\n",
    "#### Author: Yiran Jing\n",
    "#### Date: Jan 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List\n",
    "from pyspark.sql.functions import lit\n",
    "from Rank_analysis_helperfunction import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "import findspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Row\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.3 ms, sys: 18.6 ms, total: 42.9 ms\n",
      "Wall time: 3.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Build the SparkSession\n",
    "\"\"\"\n",
    "# getOrCreate(): get the current Spark session or to create one if there is none running\n",
    "# manage Spark memory limits programmatically \n",
    "# To avoid out of memory error\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Rank Model\") \\\n",
    "   .config(\"spark.executor.memory\", \"4G\")\\\n",
    "   .config('spark.driver.memory', '45G')\\\n",
    "   .config('spark.driver.maxResultSize', '10G')\\\n",
    "   .getOrCreate()\n",
    "   \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_material_change(dataset: Dataset) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    For each SKU in each store, compare the total sale with \n",
    "    the average of total sale overall stores given the same concept in the week before the last week\n",
    "    \n",
    "    return df: with column 'material_change':\n",
    "            Blank:\n",
    "                sumSales_oldWeek = 0\n",
    "            True:\n",
    "                1. for the item by Store in W1, but not in W2\n",
    "                2. avgSales_lastWeek < sumSales_oldWeek (e.g w1 > w2)\n",
    "            False:\n",
    "                the left cases. \n",
    "            \n",
    "            Note: \n",
    "                1. For the case: sumSales_oldWeek = 0, avgSales_lastWeek<0, in_W1_not_W2, material_change = ''\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the average sales of each (SKU, Concept, average sale) in the week before last week\n",
    "    average_sale_last_week = calculate_average_sale_last_week(dataset)\n",
    "    \n",
    "    # calculate last week infor\n",
    "    old_week_data = dataset.df.filter(col('Date') == dataset.week[-2])\n",
    "    \n",
    "    ## net sale \n",
    "    old_week_sale = old_week_data.withColumnRenamed(\"NetSales\", \"sumSales_oldWeek\").select('SKU',\n",
    "                                                                                           'Store',\n",
    "                                                                                           'sumSales_oldWeek')\n",
    "    # merge dataset \n",
    "    output = dataset.df.join(average_sale_last_week, on = ['SKU', 'Concept_NEW'], how = 'full')\n",
    "    output = output.join(old_week_sale, on = ['SKU', 'Store'], how = 'full')   \n",
    "    output = output.na.fill(0)\n",
    "    # test dataset\n",
    "    #test_calculate_material_change(merge_data, last_week_sale)\n",
    "    \n",
    "    # generate 'material_change' based on given condition\n",
    "    output = output.withColumn('material_change', \n",
    "                                       when(col('sumSales_oldWeek') ==0, '')\n",
    "                                       .when((col(\"sumSales_oldWeek\") > col('avgSales_lastWeek')) \n",
    "                                            & (col(\"in_W1_not_W2\") =='True'), 'True') \n",
    "                                       .otherwise('False'))\n",
    "    \n",
    "    return output\n",
    "\n",
    "#### add more test function \n",
    "def test_calculate_material_change(merge_data: pyspark.sql.dataframe.DataFrame, \n",
    "                                   last_week_sale: pyspark.sql.dataframe.DataFrame):\n",
    "    assert merge_data.count() == last_week_sale.count(), 'we want ' + str(last_week_sale.count()) + \\\n",
    "    \". But we get \"+str(merge_data.count())# test joined dataset\n",
    "    \n",
    "def calculate_unadressed_gap(dataset: Dataset) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    unadressed_gap = True if: Not shown in both W1 and W2\n",
    "            1. network_expansion = False\n",
    "            2. and in_W1_not_W2 = False\n",
    "            3. NetSale = 0\n",
    "    \"\"\"\n",
    "    output = dataset.df.withColumn('unadressed_gap', \n",
    "                                       when((col('network_expansion') =='False')&\\\n",
    "                                            (col('in_W1_not_W2') =='False')&\\\n",
    "                                            (col('NetSales') ==0), 'True')\n",
    "                                       .otherwise('False'))\n",
    "    return output\n",
    "\n",
    "def get_top_50(dataset: Dataset, weekChoice: int) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Get top 50 rank (SKU, Store) of last week or the week before last week\n",
    "    weekChoice: \n",
    "         -1: last week\n",
    "         -2: the week before last week\n",
    "    \"\"\"\n",
    "    week_data = dataset.df.filter(col('Date') == dataset.week[weekChoice])  \n",
    "    top_50_week = week_data.filter(col('rank') <=50) # top 50\n",
    "    return top_50_week.select('SKU', 'Store', 'Date')\n",
    "    \n",
    "def calculate_newcomer(dataset: Dataset) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    calculate_newcomer = True if:\n",
    "        1. Not in top 50 in the week before last week\n",
    "        2. in top 50 in the last week\n",
    "    Otherwise False\n",
    "    \"\"\"\n",
    "    top_50_last_week = get_top_50(dataset, -1)\n",
    "    top_50_week_before_last_week = get_top_50(dataset, -2).withColumnRenamed(\"Date\", \"Date_before\")\n",
    "    # Do left outer join to find new items occured in the last week\n",
    "    newcomer = top_50_last_week.join(top_50_week_before_last_week, on = ['SKU', 'Store'], how = 'left')\n",
    "    newcomer = newcomer.withColumn('newcomer',\n",
    "                                    when(col('Date_before').isNull(), 'True')\n",
    "                                   .otherwise('False')).drop(\"Date_before\").drop('Date')\n",
    "    \n",
    "    output = dataset.df.join(newcomer, on = ['SKU', 'Store'], how = 'left').fillna(\"False\", subset=['newcomer'])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.02 ms, sys: 1.46 ms, total: 3.49 ms\n",
      "Wall time: 3.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## load and clean data\n",
    "df = spark.read.csv(\"../data/rawData/data Ranking Report.csv\", header=True) # raw data from TM1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 109 ms, sys: 24.8 ms, total: 134 ms\n",
      "Wall time: 8.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create dataclass onject\n",
    "df = clean_dataset(df)\n",
    "dataset = Dataset(df = df, store_item_concept = get_store_item_concept_list(df, spark),\n",
    "                  week = get_week_list(df), concept = get_concept_list(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.1 ms, sys: 5.06 ms, total: 34.2 ms\n",
      "Wall time: 679 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset.df = calculate_network_expansion(dataset)\n",
    "dataset.df = calculate_material_change(dataset)\n",
    "dataset.df = calculate_unadressed_gap(dataset)\n",
    "dataset.df = calculate_newcomer(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SKU: string (nullable = true)\n",
      " |-- Store: string (nullable = true)\n",
      " |-- Concept_NEW: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- BusinessUnit: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- NetSales: float (nullable = false)\n",
      " |-- rank: float (nullable = false)\n",
      " |-- network_expansion: string (nullable = true)\n",
      " |-- in_W1_not_W2: string (nullable = true)\n",
      " |-- avgSales_lastWeek: double (nullable = false)\n",
      " |-- sumSales_oldWeek: float (nullable = false)\n",
      " |-- material_change: string (nullable = false)\n",
      " |-- unadressed_gap: string (nullable = false)\n",
      " |-- newcomer: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate row and unnecessary column\n",
    "dataset.df = dataset.df.drop('in_W1_not_W2','sumSales_oldWeek').dropDuplicates()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def test_no_duplicates():\n",
    "    \"\"\"\n",
    "    If we have duplicate rows in output, it means our algorithm is incorrect or inefficient\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.88 s, sys: 608 ms, total: 5.49 s\n",
      "Wall time: 4min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset.df.toPandas().to_csv('../data/output/checkresult_SKU.csv', \n",
    "                             index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
