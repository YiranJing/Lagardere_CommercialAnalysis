{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inventory Optimization Calculation\n",
    "\n",
    "This notebook is based on `EDA_InventoryOptimization.ipynb` and `CheckItems_bySubCategory.ipynb`, included all calculations within these two notebook, but NO PLOT. Just generate new CSV files.  \n",
    "\n",
    "#### Input Dataset Description\n",
    "1. `Distribution Report - Auckland Departures - July19.csv` <br/>or `../data/RawData/Distribution Report- Auckland Departures - Jan2020.csv` <br/>or `../data/RawData/Distribution Report-AKL Arrivals-Jan 2020.csv`\n",
    "2. `sales_newzealand.csv`: monthly sale data\n",
    "3. `SubCateogoryInfo.csv` \n",
    "4. `Vendor_info.csv`\n",
    "\n",
    "\n",
    "## Contents\n",
    "1. Identify items which have potential issues (Output csv in **`Output/IssuedItem` folder**)\n",
    "\n",
    "2. **Capacity adjustment**\n",
    "   - `ProposedDepth` = 3, if Depth >= 4. Otherwise empty; \n",
    "   - `VarianceDepth` = ProposedDepth - Depth (should be negative)\n",
    "2. Identify Items which satisfy the rule in at least in one month (Output csv in **`AtleastOneMonth` folder**)\n",
    "   \n",
    "3. Identify Items which satisfy the rule in every month (Output csv in **`full_month` folder**)\n",
    "\n",
    "***\n",
    "### Explaination\n",
    "#### Identify items which have potential issues\n",
    "1. `New_items`,  `Removed_items`, `Check_items` will removed from the inventory analysis: \n",
    "    1. New_items\n",
    "       - The items are not in `distribution report`, but have sale history from July-Sep or Sep-Nov\n",
    "       - save them to `new_SKU.csv`\n",
    "    2. Removed_items\n",
    "       - The items are in `distribution report`, but have no sales from July-Sep or Sep-Nov\n",
    "       - save them to `removed_SKU.csv`\n",
    "    3. Check_items \n",
    "       - The items are in `distribution report`, but have no sales from Apr-Sep or Sep-Nov\n",
    "       - save them to `check_SKU.csv`\n",
    "       - There might be two reasons for this collection\n",
    "          1. file has not been update\n",
    "          2. the items are truly in the store, but cannot sold out \n",
    "    4. Incorrect_record_items\n",
    "       - The items has extremely high ratio of capacity/facing. (>6)\n",
    "       - Either facing or capacity is incorrect. Need to report to store manager.\n",
    "       - Save them in `Incorrect_record_items.csv`\n",
    "    5. Items Depth < 2 (same as Capacity < Facings *2)\n",
    "       - e.g. Capacity = Facings = 1, incorrect\n",
    "       - e.g Facing = 3, Capacity = 5, incorrect\n",
    "       - Capacity should > Facing. \n",
    "       - Save them in `Depth2_items.csv`\n",
    "       \n",
    "#### At least one month SKU\n",
    "Items which satisfy the rule in at least in one month from July, to Sep.\n",
    "- adjusted SKU(atLeastOneMonth): items which depth >=4, and has new proposed depth. save in `adjusted_SKU.csv`\n",
    "- unadjusted SKU(atLeastOneMonth): items which depth <4, and has no influnece under new rules. save in `unadjusted_SKU.csv`\n",
    "\n",
    "#### Full month (3 month) SKU\n",
    "Items satisfy the rule in every month (e.g. July, Aug, Sep)\n",
    "Save in `qty_less_than_capacity_all_month_SKU.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolving problems:\n",
    "### Resolving problem 1: SKU sold only one month\n",
    "For the SKU sold only one month (SKU within `atLeastOneMonth` folder, whose `stdQty_std_by_SKU` standardard derivation of each SKU is empty), what's the reason? removed SKU or OOS issue?\n",
    "\n",
    "### Resolving problem 2: Replace Strategy\n",
    "For the SKU with high `Capacity_to_avg_qty`, and low `StdQty_to_AvgQty`(risk) \n",
    "\n",
    "There are three posible strategies we can consider\n",
    "1. `proposed depth`, given facing, for the SKU's depth >4, adjust to 3. (easy)\n",
    "2. `proposed #POGS`, given SKU's depth <=3, and `#POGS` >1, adjust to 1. \n",
    "3. `proposed facings`, given SKU's depth <=3, POGS =1 and facing is relative large. (hard!)\n",
    "#### Notes:\n",
    "for 2 and 3 above, we need to find **underspace SKU** at the same time. (Reverse Logic + Martix design)\n",
    "\n",
    "#### Martix design\n",
    "To make decision in Replace Strategy (especially `proposed facings`) we need consider some Key factors: \n",
    "1. `Capacity_to_avg_qty`: measure sales performance over certain period\n",
    "2. `StdQty_to_AvgQty`: **measure the risk**\n",
    "3. `Classification`: Historical popularity \n",
    "4. `Linear (cm)`: match space\n",
    "5. `Total Margin ($)`\n",
    "6. `SubCategory`: similar or same.\n",
    "The real decision should based on these factors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local appName=Inventory Opti Model>\n",
      "2.4.4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from Inventory_opti_helperFunction import *\n",
    "import findspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "import re\n",
    "import errno, sys\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "\"\"\"\n",
    "Build the SparkSession\n",
    "\"\"\"\n",
    "# getOrCreate(): get the current Spark session or to create one if there is none running\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Inventory Opti Model\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "   \n",
    "sc = spark.sparkContext # create a SparkSession object from your SparkContext\n",
    "\n",
    "# Verify SparkContext\n",
    "print(sc)\n",
    "\n",
    "# Print Spark version\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ŒNotes:\n",
      "1. The sales_newzealand.csv must have the sales data from the store you want to test: (NZDF.AKL108 or NZDF.AKL109 or both)\n",
      "\n",
      "2. The decision of start date is based on the \"generating date of Distribution Report\".\n",
      "\n",
      "3. If start date is 2019-04-01, then end_date will be 2019-10-01, the month begin to test is July. \n",
      "   If start date is 2019-06-01, then end_date will be 2019-12-01, the month begin to test is Sep. \n",
      "   \n",
      "4. The file name of Distribution Report must contain 'Departures' or 'Arrivals':\n",
      "   Departures: NZDF.AKL108\n",
      "   Arrivals: NZDF.AKL109\n",
      "   \n",
      "5. The output folder name is â€˜Outputâ€™ only. Remeber rename it to distinguish different input data: \n",
      "   for example: Rename to Output_Sep_108, Output_July_108, Output_Sep_109\n",
      "   \n",
      "    \n",
      "Enter the path of 6 month sales data, \n",
      "    e.g. ../data/RawData/sales_newzealand.csv:../data/RawData/sales_newzealand.csv\n",
      "Enter the path of disrtibuted report data, \n",
      "    e.g. ../data/RawData/Distribution Report - Auckland Departures - July19.csv or ../data/RawData/Distribution Report- Auckland Departures - Jan2020.csv or ../data/RawData/Distribution Report-AKL Arrivals-Jan 2020.csv:../data/RawData/Distribution Report-AKL Arrivals-Jan 2020.csv\n",
      "Store name is NZDF.AKL109\n",
      "Input start date, 2019-04-01 or 2019-06-01: 2019-06-01\n",
      "end date is 2019-12-01,  the month begin to test is 9\n",
      "\n",
      "\n",
      "Find 0 Removed items, save them in Output/IssuedItem/removed_SKU.csv\n",
      "Find 161 New items, save them in Output/IssuedItem/new_SKU.csv\n",
      "Find 25 checked items, save them in Output/IssuedItem/checked_SKU.csv\n",
      "Find 44 Incorrect_record_items, save them in Output/IssuedItem/Incorrect_record_items.csv\n",
      "Find 0 Depth < 2 items, save them in Output/IssuedItem/Depth2_items.csv\n",
      "Save adjusted SKU(atLeastOneMonth) to Output/atLeastOneMonth/adjusted_SKU.csv\n",
      "Save unadjusted SKU(atLeastOneMonth) to Output/atLeastOneMonth/unadjusted_SKU.xlsx\n",
      "Save fullMonth SKU to Output/fullMonth/qty_less_than_capacity_all_month_SKU.csv\n",
      "\n",
      "\n",
      "Finish!\n"
     ]
    }
   ],
   "source": [
    "### User Input from interface\n",
    "sales_data_path, dist_data_path, start_date, end_date, split_month, store_name = user_put()\n",
    "\n",
    "#################################\n",
    "## read, merge and clean datasets\n",
    "#################################\n",
    "month_merge, month_merge_early, month_merge_late, dist_df = read_merge_and_clean_data(sales_data_path, \n",
    "                                                                             dist_data_path, split_month, \n",
    "                                                                             start_date, end_date, store_name, spark)\n",
    "\n",
    "##############################################\n",
    "## Identify items with potential issues\n",
    "##############################################\n",
    "\n",
    "df = Identify_and_output_issused_items(month_merge, month_merge_early, month_merge_late, dist_df)\n",
    "df.toPandas().to_csv('../data/CleanedData/month_merge_total.csv', \n",
    "                             index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "#############################\n",
    "### Analysis\n",
    "#############################\n",
    "names = [\"totalMonthlyNetSale\", \"totalMonthlyQtySold\", \"Price\", \n",
    "         \"SellMargin\", \"Facings\", \"Capacity\"]\n",
    "df = convertColumn(df, names, FloatType())\n",
    "\n",
    "\"\"\"\n",
    "Add more columns\n",
    "\"\"\"\n",
    "# add mean, std and geometric mean of qty sold GROUPBY subcategory and month to datatset \n",
    "df = calculate_mean_std_and_geometric_mean(df)\n",
    "# add capacity/sale ratio to dataset\n",
    "df = calculate_Capacity_to_sales(df)\n",
    "# add depth, proposed depth\n",
    "df = calculate_Depths(df)\n",
    "\n",
    "# find and analysis atLeastOneMonth_SKU\n",
    "# df_full is the combination of unchanged_SKU and changed_SKU\n",
    "df_atLeastOneMonth, unchanged_SKU, changed_SKU, df_full = find_and_analysis_atLeastOneMonth_SKU(df)\n",
    "Group_and_save_atLeastOneMonth_SKU(unchanged_SKU, changed_SKU)\n",
    "\n",
    "# find and analysis fullMonth SKU \n",
    "full_month_SKU_info = find_and_analysis_fullMonth_SKU(df_atLeastOneMonth, split_month, spark)\n",
    "save_fullMonth_SKU(full_month_SKU_info)\n",
    "\n",
    "print(\"\\n\\nFinish!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_full.toPandas().to_csv('../data/Output/atLeastOneMonth/AllatLeastOneMonth_SKU.csv', \n",
    "                                 index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
